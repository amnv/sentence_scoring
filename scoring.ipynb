{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from py_stringmatching.similarity_measure.soft_tfidf import SoftTfIdf\n",
    "from py_stringmatching.similarity_measure.jaro_winkler import JaroWinkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_string_matching(cleaned_text, tuples):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    # add words from tuple to corpus\n",
    "    for tupl in tuples:\n",
    "        corpus.append(word_tokenize(tupl['prop'] + \" \" + tupl['value']))\n",
    "    \n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "    \n",
    "    # add words from sentences to corpus\n",
    "    for sentence in sentences:\n",
    "        tokenized_sent = word_tokenize(sentence)\n",
    "        corpus.append(tokenized_sent)\n",
    "    \n",
    "    return SoftTfIdf(corpus, sim_func = JaroWinkler().get_raw_score, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sliding_window(tokens, size):\n",
    "    for i in range(len(tokens) - size + 1):\n",
    "        yield tokens[i: i + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned text and a dict with keys prop and value)\n",
    "def score_sentences(cleaned_text, tupl, soft_tfidf):\n",
    "    \n",
    "    tuple_tokens = word_tokenize(tupl['prop'] + \" \" + tupl['value'])\n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "    \n",
    "    scores = []\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = word_tokenize(sentence)\n",
    "        \n",
    "        big_score = 0.0\n",
    "        for window in token_sliding_window(sentence_tokens, 5):\n",
    "            score = soft_tfidf.get_raw_score(window, tuple_tokens)\n",
    "            if score > big_score:\n",
    "                big_score = score\n",
    "            \n",
    "        scores.append({'sent':sentence, 'score': big_score})\n",
    "    \n",
    "    return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_bigger_score(scores):\n",
    "    return max(scores, key=lambda s:s['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence_and_set_ner(cleaned_text, tupl, soft_tfidf, threshold=0.6):\n",
    "    \n",
    "    scores = score_sentences(cleaned_text, tupl, soft_tfidf)\n",
    "    \n",
    "    #big_score = select_bigger_score(scores)\n",
    "    big_score = scores\n",
    "\n",
    "    propTokens = word_tokenize(tupl['prop'])\n",
    "    valueTokens = word_tokenize(tupl['value'])\n",
    "    \n",
    "    selected_sentence = []\n",
    "    list_sentence_tokens = []\n",
    "    list_ner = []\n",
    "    for i in scores:\n",
    "        if i[\"score\"] > threshold:\n",
    "            selected_sentence.append(i)\n",
    "            sentence_tokens = word_tokenize(i['sent'])\n",
    "            list_sentence_tokens.append(sentence_tokens)\n",
    "\n",
    "            # get ner annotations according to soft tf-idf measure\n",
    "            kept_index_prop = [-1] * len(propTokens)\n",
    "            bigger_token_score = [0.0] * len(propTokens)\n",
    "            for i, prop_token in enumerate(propTokens):\n",
    "                for j, token in enumerate(sentence_tokens):\n",
    "                    score = soft_tfidf.get_raw_score([prop_token], [token])\n",
    "                    if score > bigger_token_score[i]:\n",
    "                        bigger_token_score[i] = score\n",
    "                        kept_index_prop[i] = j\n",
    "\n",
    "            kept_index_value = [-1] * len(valueTokens)\n",
    "            bigger_value_score = [0.0] * len(valueTokens)\n",
    "            for i, value_token in enumerate(valueTokens):\n",
    "                for j, token in enumerate(sentence_tokens):\n",
    "                    score = soft_tfidf.get_raw_score([value_token], [token])\n",
    "                    if score > bigger_value_score[i]:\n",
    "                        bigger_value_score[i] = score\n",
    "                        kept_index_value[i] = j\n",
    "\n",
    "            ner = [''] * len(sentence_tokens)\n",
    "            for index, token in enumerate(sentence_tokens):\n",
    "                if index in kept_index_prop:\n",
    "                    ner[index] = 'PROP'\n",
    "                    continue\n",
    "                if index in kept_index_value:\n",
    "                    ner[index] = 'VALUE'\n",
    "                    continue\n",
    "                ner[index] = 'O'\n",
    "            list_ner.append(ner)\n",
    "\n",
    "    \n",
    "    \n",
    "    return selected_sentence, list_sentence_tokens, list_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(text_dir, structured_dir, filename):\n",
    "    \n",
    "    content = open(text_dir + filename, 'r') \n",
    "    text = content.read()\n",
    "    text = text.replace(\"\\n\", \" \").strip().rstrip()\n",
    "    \n",
    "    tuples = []\n",
    "    content2 = open(structured_dir + filename, 'r')\n",
    "    for line in content2:\n",
    "        items = line.replace(\"\\n\",\"\").replace(\"_\",\" \").split(\"\\t:\\t\")\n",
    "        tuples.append({'prop': items[0], 'value': items[1]})\n",
    "    \n",
    "    return text, tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file_tuple(filename, big_score, sentence_tokens, ner):\n",
    "    filename_output = \"out/\" + filename\n",
    "    with open(filename_output, \"w\") as out:\n",
    "        for i in range(len(big_score)):\n",
    "#             out.write(big_score[i]['sent'] + \"\\n\\n\" +\n",
    "#             \"score: \" + str(big_score[i]['score']) + \"\\n\\n\")\n",
    "            \n",
    "            add_coma = False\n",
    "            for j in list(zip(sentence_tokens[i], ner[i])):\n",
    "                if add_coma:\n",
    "                    out.write(\",\")\n",
    "                else: add_coma = True\n",
    "                line = \", \".join(str(x) for x in j)\n",
    "                out.write(\"(\" + line + \")\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "def write_to_file_xml(filename, big_score, sentence_tokens, ner):\n",
    "    filename_output = \"out/\" + filename.replace(\".txt\", \".xml\")\n",
    "    with open(filename_output, \"w\") as out:\n",
    "        is_sequence = False # caso a palavra anterior tbm seja uma cell type\n",
    "        \n",
    "        out.write('<document id=\"{0}\">\\n'.format(filename.split(\".\")[0]))\n",
    "        for i in range(len(big_score)):\n",
    "            out.write(\"\\t<sentence>\")\n",
    "            for j in list(zip(sentence_tokens[i], ner[i])):\n",
    "                if j[1] == \"O\":\n",
    "                    if is_sequence:\n",
    "                        out.write(\"</cell_type> \" + j[0])\n",
    "                        is_sequence = False\n",
    "                    else:\n",
    "                        out.write(j[0] + \" \")\n",
    "                else:\n",
    "                    if is_sequence:\n",
    "                        out.write(\" {0}\".format(j[0]))\n",
    "                    else:\n",
    "                        out.write(\"<cell_type>{0}\".format(j[0]))\n",
    "                        is_sequence = True\n",
    "            if is_sequence:\n",
    "                out.write(\"</cell_type>\")\n",
    "                is_sequence = False\n",
    "            out.write(\"</sentence>\\n\")\n",
    "        out.write(\"</document>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prop': '', 'value': 'adipose tissue'}\n",
      "Término escrita arquivo adipose_tissue.txt\n",
      "\n",
      "{'prop': '', 'value': 'bone marrow'}\n",
      "Término escrita arquivo bone_marrow.txt\n",
      "\n",
      "{'prop': '', 'value': 'umbilical cord'}\n",
      "Término escrita arquivo umbilical_cord.txt\n",
      "\n",
      "{'prop': '', 'value': 'epithelial'}\n",
      "Término escrita arquivo epithelial.txt\n",
      "\n",
      "{'prop': '', 'value': 'fibroblast'}\n",
      "Término escrita arquivo fibroblast.txt\n",
      "\n",
      "{'prop': '', 'value': 'kidney'}\n",
      "Término escrita arquivo kidney.txt\n",
      "\n",
      "{'prop': '', 'value': 'neural cell'}\n",
      "Término escrita arquivo neural_cell.txt\n",
      "\n",
      "{'prop': '', 'value': 'precursor cell'}\n",
      "Término escrita arquivo precursor_cell.txt\n",
      "\n",
      "{'prop': '', 'value': 'stem cell'}\n",
      "Término escrita arquivo stem_cell.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_dir = 'data/text/'\n",
    "structured_dir = 'data/structured_data/'\n",
    "# filenames = ['Abbeville_County,_South_Carolina', 'Acadia_Parish,_Louisiana', 'Accomack_County,_Virginia']\n",
    "filenames = ['adipose_tissue.txt', \n",
    "             'bone_marrow.txt', \n",
    "             \"umbilical_cord.txt\",\n",
    "             \"epithelial.txt\",\n",
    "             \"fibroblast.txt\",\n",
    "             \"kidney.txt\",\n",
    "             \"neural_cell.txt\",\n",
    "             \"precursor_cell.txt\",\n",
    "             \"stem_cell.txt\"]\n",
    "\n",
    "for filename in filenames:\n",
    "    # filename = filenames[2]\n",
    "    text, tuples = read_files(text_dir, structured_dir, filename)\n",
    "\n",
    "    soft_tfidf = instantiate_string_matching(text, tuples)\n",
    "\n",
    "    for tupl in tuples:\n",
    "        print(tupl)\n",
    "\n",
    "        # Only the big score of the sentence\n",
    "        # scores = score_sentence(text, tupl, soft_tfidf)\n",
    "        # print(select_bigger_score(scores))\n",
    "\n",
    "        # The big sentence score inside the window + named entity tag for each sentence token\n",
    "        big_score, sentence_tokens, ner = score_sentence_and_set_ner(text, tupl, soft_tfidf)\n",
    "\n",
    "        filename_output = \"out/\" + filename\n",
    "#         for i in range(len(big_score)):\n",
    "#             print(big_score[i]['sent'])\n",
    "#             print(big_score[i]['score'])\n",
    "#             print(list(zip(sentence_tokens[i], ner[i])))\n",
    "\n",
    "        write_to_file_tuple(filename, big_score, sentence_tokens, ner)\n",
    "        print(\"Término escrita arquivo {0}\".format(filename))\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
